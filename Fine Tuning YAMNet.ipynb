{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\naman\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\naman\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow_estimator\\python\\estimator\\util.py:74: The name tf.train.SessionRunHook is deprecated. Please use tf.estimator.SessionRunHook instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\naman\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow_hub\\native_module.py:92: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\naman\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow_hub\\saved_model_module.py:40: The name tf.saved_model.constants.LEGACY_INIT_OP_KEY is deprecated. Please use tf.compat.v1.saved_model.constants.LEGACY_INIT_OP_KEY instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import soundfile as sf\n",
    "import tensorflow_hub as hub\n",
    "import warnings\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing dataset for trianing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['azaspi1', 'chcant2', 'houspa', 'redcro', 'wbwwre1']\n"
     ]
    }
   ],
   "source": [
    "train_dir='Data/train/'\n",
    "test_dir='Data/test/'\n",
    "\n",
    "classes=os.listdir(train_dir)\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dataset_dir):\n",
    "    x=[]\n",
    "    y=[]\n",
    "    label=0\n",
    "    for (dir,folder,filenames) in os.walk(dataset_dir):\n",
    "        if filenames:\n",
    "            print(dir,label,len(filenames))\n",
    "            for i in filenames:\n",
    "                file_dir=os.path.join(dir,i)\n",
    "                audio = tf.io.read_file(file_dir)\n",
    "                audio, sr = tf.audio.decode_wav(audio,\n",
    "                                                desired_channels=1,\n",
    "                                                desired_samples=44100)\n",
    "                # audio = tfio.audio.resample(audio, 44100, 16000)\n",
    "\n",
    "                # audio=tf.reshape(audio,(1,-1))\n",
    "                # audio=tf.squeeze(audio)\n",
    "                \n",
    "                audio = tf.squeeze(audio, axis=-1)\n",
    "                \n",
    "                x.append(audio)\n",
    "                y.append(label)\n",
    "            label+=1\n",
    "    return tf.convert_to_tensor(x),tf.convert_to_tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data/train/azaspi1 0 40\n",
      "Data/train/chcant2 1 47\n",
      "Data/train/houspa 2 59\n",
      "Data/train/redcro 3 34\n",
      "Data/train/wbwwre1 4 61\n",
      "Data/test/azaspi1 0 13\n",
      "Data/test/chcant2 1 10\n",
      "Data/test/houspa 2 11\n",
      "Data/test/redcro 3 6\n",
      "Data/test/wbwwre1 4 13\n"
     ]
    }
   ],
   "source": [
    "x_train,y_train=load_dataset(train_dir)\n",
    "x_test,y_test=load_dataset(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([241, 44100])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Pre-trained YAMNET model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\naman\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow_hub\\resolver.py:120: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\naman\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow_hub\\resolver.py:120: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\naman\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow_hub\\module_v2.py:120: The name tf.saved_model.load_v2 is deprecated. Please use tf.compat.v2.saved_model.load instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\naman\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow_hub\\module_v2.py:120: The name tf.saved_model.load_v2 is deprecated. Please use tf.compat.v2.saved_model.load instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "yamnet_model_handle = 'https://tfhub.dev/google/yamnet/1'\n",
    "yamnet_model = hub.load(yamnet_model_handle)\n",
    "\n",
    "def get_features(audio):\n",
    "    _,features,_=yamnet_model(audio)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Features from pre-trained model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(241, 5, 1024) (53, 5, 1024)\n"
     ]
    }
   ],
   "source": [
    "x_train_feat=tf.convert_to_tensor([get_features(i) for i in x_train])\n",
    "x_test_feat=tf.convert_to_tensor([get_features(i) for i in x_test])\n",
    "print(x_train_feat.shape,x_test_feat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([241, 5, 1024]), TensorShape([53, 5, 1024]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_feat=tf.squeeze(x_train_feat)\n",
    "x_test_feat=tf.squeeze(x_test_feat)\n",
    "x_train_feat.shape,x_test_feat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\naman\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\naman\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 5, 512)            524800    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 5, 64)             32832     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 320)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 5)                 1605      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 559237 (2.13 MB)\n",
      "Trainable params: 559237 (2.13 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "my_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(5,1024), dtype=tf.float32,\n",
    "                          name='input_embedding'),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(len(classes))\n",
    "], name='my_model')\n",
    "\n",
    "my_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                 optimizer=\"adam\",\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',\n",
    "                                            patience=3,\n",
    "                                            restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "WARNING:tensorflow:From c:\\Users\\naman\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\naman\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 1s 53ms/step - loss: 1.4495 - accuracy: 0.4191 - val_loss: 1.8716 - val_accuracy: 0.5283\n",
      "Epoch 2/20\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.8915 - accuracy: 0.6971 - val_loss: 1.6115 - val_accuracy: 0.6981\n",
      "Epoch 3/20\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.6162 - accuracy: 0.7842 - val_loss: 1.4631 - val_accuracy: 0.7358\n",
      "Epoch 4/20\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.4655 - accuracy: 0.8465 - val_loss: 1.4940 - val_accuracy: 0.7925\n",
      "Epoch 5/20\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.3479 - accuracy: 0.8921 - val_loss: 1.4777 - val_accuracy: 0.7736\n",
      "Epoch 6/20\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.2703 - accuracy: 0.9419 - val_loss: 1.5671 - val_accuracy: 0.7925\n",
      "Epoch 7/20\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.2039 - accuracy: 0.9585 - val_loss: 1.5936 - val_accuracy: 0.7925\n"
     ]
    }
   ],
   "source": [
    "history = my_model.fit(x_train_feat,y_train,\n",
    "                         #    batch_size=1,\n",
    "                            epochs=20,\n",
    "                        validation_data=(x_test_feat,y_test),\n",
    "                        verbose=1,\n",
    "                        callbacks=[callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 118ms/step\n",
      "The main sound is: chcant2\n"
     ]
    }
   ],
   "source": [
    "audio = tf.io.read_file('Data/train/chcant2/XC118040.wav')\n",
    "audio, sr = tf.audio.decode_wav(audio,\n",
    "                                desired_channels=1,\n",
    "                                desired_samples=44100)\n",
    "audio=tf.squeeze(audio,axis=-1)\n",
    "feature=get_features(audio)\n",
    "embeddings=tf.convert_to_tensor([feature])\n",
    "result = my_model.predict(embeddings)\n",
    "\n",
    "inferred_class = classes[result.mean(axis=0).argmax()]\n",
    "print(f'The main sound is: {inferred_class}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting to single model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReduceMeanLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, axis=0, **kwargs):\n",
    "    super(ReduceMeanLayer, self).__init__(**kwargs)\n",
    "    self.axis = axis\n",
    "\n",
    "  def call(self, input):\n",
    "    return tf.math.reduce_mean(input, axis=self.axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_segment = tf.keras.layers.Input(shape=(), dtype=tf.float32, name='audio')\n",
    "# embedding_extraction_layer = hub.KerasLayer(yamnet_model_handle,\n",
    "#                                             trainable=False, name='yamnet')\n",
    "# yamnet_model_handle = 'https://tfhub.dev/google/yamnet/1'\n",
    "# yamnet_model = hub.load(yamnet_model_handle)\n",
    "yamnet_model=hub.KerasLayer(yamnet_model,trainable=False,name='yamnet')\n",
    "_, embeddings_output, _ = yamnet_model(input_segment)\n",
    "my_model=hub.KerasLayer(my_model,trainable=False,name='my_model')\n",
    "serving_outputs = my_model(tf.convert_to_tensor([embeddings_output]))\n",
    "serving_outputs = ReduceMeanLayer(axis=0, name='classifier')(serving_outputs)\n",
    "serving_model = tf.keras.Model(input_segment, serving_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " audio (InputLayer)          [(None,)]                 0         \n",
      "                                                                 \n",
      " yamnet (KerasLayer)         [(None, 521),             0         \n",
      "                              (None, 1024),                      \n",
      "                              (None, 64)]                        \n",
      "                                                                 \n",
      " tf.convert_to_tensor (TFOp  (1, None, 1024)           0         \n",
      " Lambda)                                                         \n",
      "                                                                 \n",
      " my_model (KerasLayer)       (1, 5)                    559237    \n",
      "                                                                 \n",
      " classifier (ReduceMeanLaye  (5,)                      0         \n",
      " r)                                                              \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 559237 (2.13 MB)\n",
      "Trainable params: 0 (0.00 Byte)\n",
      "Non-trainable params: 559237 (2.13 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "serving_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing final model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5,), dtype=float32, numpy=\n",
       "array([-1.2167687, -3.430489 ,  2.9077516, -0.7017541,  1.819451 ],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio = tf.io.read_file('Data/train/houspa/XC112666.wav')\n",
    "audio, sr = tf.audio.decode_wav(audio,\n",
    "                                desired_channels=1,\n",
    "                                desired_samples=44100)\n",
    "audio=tf.squeeze(audio,axis=-1)\n",
    "serving_model(audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the Final Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/assets\n"
     ]
    }
   ],
   "source": [
    "serving_model.save('model/', include_optimizer=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the saved model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "reloaded_model = tf.saved_model.load('model/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.2167687 -3.430489   2.9077516 -0.7017541  1.819451 ]\n",
      "The main sound is: houspa\n"
     ]
    }
   ],
   "source": [
    "result=reloaded_model(audio)\n",
    "result=np.array(result)\n",
    "print(result)\n",
    "print(f'The main sound is: {classes[result.argmax()]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting to tflite model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def get_file_size(file_path):\n",
    "    size = os.path.getsize(file_path)\n",
    "    return round(size/(1024*1024),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\naman\\AppData\\Local\\Temp\\tmpu05yn83y\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\naman\\AppData\\Local\\Temp\\tmpu05yn83y\\assets\n"
     ]
    }
   ],
   "source": [
    "tflite_model='model.tflite'\n",
    "converter=tf.lite.TFLiteConverter.from_keras_model(reloaded_model)\n",
    "lite_model=converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15138704"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open(tflite_model,\"wb\").write(lite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.437"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_file_size(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducing further\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\naman\\AppData\\Local\\Temp\\tmpxf6_pil5\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\naman\\AppData\\Local\\Temp\\tmpxf6_pil5\\assets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3993888"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name='model_quant.tflite'\n",
    "converter=tf.lite.TFLiteConverter.from_keras_model(reloaded_model)\n",
    "converter.optimizations=[tf.lite.Optimize.DEFAULT]\n",
    "# converter.target_spec.supported_types=[tf.float]\n",
    "lite_model=converter.convert()\n",
    "open(name,\"wb\").write(lite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.809"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_file_size(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing quantized model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_model=tf.lite.Interpreter(model_path=name)\n",
    "tf_model.resize_tensor_input(tf_model.get_input_details()[0]['index'], [44100])\n",
    "tf_model.allocate_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'serving_default_audio:0', 'index': 0, 'shape': array([44100]), 'shape_signature': array([-1]), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
      "[{'name': 'StatefulPartitionedCall:0', 'index': 225, 'shape': array([5]), 'shape_signature': array([5]), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n"
     ]
    }
   ],
   "source": [
    "input_details=tf_model.get_input_details()\n",
    "output_details=tf_model.get_output_details()\n",
    "print(input_details)\n",
    "print(output_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([44100])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio = tf.io.read_file('Data/test/wbwwre1/XC519209.wav')\n",
    "audio, sr = tf.audio.decode_wav(audio,\n",
    "                                desired_channels=1,\n",
    "                                desired_samples=44100)\n",
    "audio=tf.squeeze(audio,axis=-1)\n",
    "audio.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The main sound is: wbwwre1\n"
     ]
    }
   ],
   "source": [
    "tf_model.set_tensor(input_details[0]['index'],audio)\n",
    "tf_model.invoke()\n",
    "prediction=tf_model.get_tensor(output_details[0]['index'])\n",
    "print(f'The main sound is: {classes[prediction.argmax()]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
